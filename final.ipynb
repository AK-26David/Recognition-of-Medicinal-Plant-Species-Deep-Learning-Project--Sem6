{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computer Vision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-image version: 0.24.0\n",
      "Frangi function parameters:\n",
      "(image, sigmas=range(1, 10, 2), scale_range=None, scale_step=None, alpha=0.5, beta=0.5, gamma=None, black_ridges=True, mode='reflect', cval=0)\n",
      "For more details, refer to the documentation at:\n",
      "https://scikit-image.org/docs/stable/api/skimage.filters.html#skimage.filters.frangi\n",
      "Found 1835 images to process\n",
      "Processed 0/1835 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0b/d5vghlcn39jb_kt0c4whkzcr0000gn/T/ipykernel_7610/2342296929.py:39: UserWarning: Use keyword parameter `sigmas` instead of `scale_range` and `scale_range` which will be removed in version 0.17.\n",
      "  venation_map = frangi(clahe_image,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100/1835 images\n",
      "Processed 200/1835 images\n",
      "Processed 300/1835 images\n",
      "Processed 400/1835 images\n",
      "Processed 500/1835 images\n",
      "Processed 600/1835 images\n",
      "Processed 700/1835 images\n",
      "Processed 800/1835 images\n",
      "Processed 900/1835 images\n",
      "Processed 1000/1835 images\n",
      "Processed 1100/1835 images\n",
      "Processed 1200/1835 images\n",
      "Processed 1300/1835 images\n",
      "Processed 1400/1835 images\n",
      "Processed 1500/1835 images\n",
      "Processed 1600/1835 images\n",
      "Processed 1700/1835 images\n",
      "Processed 1800/1835 images\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.filters import frangi\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "\n",
    "def preprocess_leaf_image(image_path, output_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Preprocess a leaf image to extract RGB, venation map, and edge map\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the leaf image\n",
    "        output_size (tuple): Size to resize images to (height, width)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (RGB image, venation map, edge map)\n",
    "    \"\"\"\n",
    "    # Read the original image\n",
    "    original_image = cv2.imread(image_path)\n",
    "    if original_image is None:\n",
    "        raise ValueError(f\"Could not read image at {image_path}\")\n",
    "    \n",
    "    # 1. RGB Image Extraction\n",
    "    rgb_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "    rgb_image = cv2.resize(rgb_image, output_size)\n",
    "    \n",
    "    # 2. Venation Map Extraction\n",
    "    # Convert to grayscale\n",
    "    gray_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2GRAY)\n",
    "    gray_image = cv2.resize(gray_image, output_size)\n",
    "    \n",
    "    # Apply CLAHE (Contrast Limited Adaptive Histogram Equalization)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    clahe_image = clahe.apply(gray_image)\n",
    "    \n",
    "    # Apply Frangi filter to enhance vein-like structures\n",
    "    # Updated parameters to be compatible with current scikit-image version\n",
    "    venation_map = frangi(clahe_image, \n",
    "                         scale_range=(1, 3), \n",
    "                         scale_step=0.5,\n",
    "                         beta=15,  # Using beta instead of beta1/beta2\n",
    "                         black_ridges=False)\n",
    "    \n",
    "    # Normalize the venation map\n",
    "    venation_map = cv2.normalize(venation_map, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "    \n",
    "    # 3. Edge Map Extraction using Canny edge detection\n",
    "    edge_map = cv2.Canny(gray_image, 50, 150)\n",
    "    \n",
    "    return rgb_image, venation_map, edge_map\n",
    "\n",
    "def process_dataset(dataset_path, output_dir):\n",
    "    \"\"\"\n",
    "    Process an entire dataset of leaf images\n",
    "    \n",
    "    Args:\n",
    "        dataset_path (str): Path to the dataset directory\n",
    "        output_dir (str): Directory to save processed images\n",
    "    \"\"\"\n",
    "    # Create output directories if they don't exist\n",
    "    os.makedirs(os.path.join(output_dir, 'rgb'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'venation'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'edge'), exist_ok=True)\n",
    "    \n",
    "    # Get all image files\n",
    "    image_extensions = ['*.jpg', '*.jpeg', '*.png']\n",
    "    image_files = []\n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(glob(os.path.join(dataset_path, '**', ext), recursive=True))\n",
    "    \n",
    "    print(f\"Found {len(image_files)} images to process\")\n",
    "    \n",
    "    for idx, image_path in enumerate(image_files):\n",
    "        try:\n",
    "            # Extract class from path (assuming dataset structure: dataset/class/image.jpg)\n",
    "            # Handle paths with spaces correctly\n",
    "            path_parts = os.path.normpath(image_path).split(os.sep)\n",
    "            class_name = path_parts[-2]  # Assuming last directory is the class name\n",
    "            file_name = os.path.basename(image_path)\n",
    "            \n",
    "            # Create class directories if they don't exist\n",
    "            os.makedirs(os.path.join(output_dir, 'rgb', class_name), exist_ok=True)\n",
    "            os.makedirs(os.path.join(output_dir, 'venation', class_name), exist_ok=True)\n",
    "            os.makedirs(os.path.join(output_dir, 'edge', class_name), exist_ok=True)\n",
    "            \n",
    "            # Process the image\n",
    "            rgb_image, venation_map, edge_map = preprocess_leaf_image(image_path)\n",
    "            \n",
    "            # Save processed images\n",
    "            cv2.imwrite(os.path.join(output_dir, 'rgb', class_name, file_name), \n",
    "                        cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR))\n",
    "            cv2.imwrite(os.path.join(output_dir, 'venation', class_name, file_name), venation_map)\n",
    "            cv2.imwrite(os.path.join(output_dir, 'edge', class_name, file_name), edge_map)\n",
    "            \n",
    "            if idx % 100 == 0:\n",
    "                print(f\"Processed {idx}/{len(image_files)} images\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_path}: {e}\")\n",
    "\n",
    "def visualize_preprocessing(image_path):\n",
    "    \"\"\"\n",
    "    Visualize the preprocessing steps for a single image\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the leaf image\n",
    "    \"\"\"\n",
    "    rgb_image, venation_map, edge_map = preprocess_leaf_image(image_path)\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(rgb_image)\n",
    "    plt.title('RGB Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(venation_map, cmap='gray')\n",
    "    plt.title('Venation Map')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(edge_map, cmap='gray')\n",
    "    plt.title('Edge Map')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Function to check scikit-image version and available parameters\n",
    "def check_frangi_parameters():\n",
    "    \"\"\"\n",
    "    Print the available parameters for the frangi function\n",
    "    \"\"\"\n",
    "    import inspect\n",
    "    from skimage import __version__ as skimage_version\n",
    "    \n",
    "    print(f\"scikit-image version: {skimage_version}\")\n",
    "    print(\"Frangi function parameters:\")\n",
    "    print(inspect.signature(frangi))\n",
    "    print(\"For more details, refer to the documentation at:\")\n",
    "    print(\"https://scikit-image.org/docs/stable/api/skimage.filters.html#skimage.filters.frangi\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Check frangi parameters to diagnose issues\n",
    "    check_frangi_parameters()\n",
    "    \n",
    "    # Path to the Mendeley leaf dataset\n",
    "    dataset_path = \"/Users/arnavkarnik/Documents/Recognition-of-Medicinal-Plant-Species-Deep-Learning-Project--Sem6/Medicinal Leaf Dataset/Segmented Medicinal Leaf Images\"\n",
    "\n",
    "    \n",
    "    # Process the entire dataset\n",
    "    process_dataset(dataset_path, output_dir)\n",
    "    \n",
    "    # Alternatively, visualize preprocessing for a single image\n",
    "    # sample_image = \"/Users/arnavkarnik/Documents/Recognition-of-Medicinal-Plant-Species-Deep-Learning-Project--Sem6/Medicinal Leaf Dataset/Segmented Medicinal Leaf Images/Punica Granatum (Pomegranate)/PG-S-022.jpg\"\n",
    "    # visualize_preprocessing(sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/arnavkarnik/Documents/Recognition-of-Medicinal-Plant-Species-Deep-Learning-Project--Sem6/Output/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 27\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Image Data Augmentation\u001b[39;00m\n\u001b[1;32m     17\u001b[0m datagen \u001b[38;5;241m=\u001b[39m ImageDataGenerator(\n\u001b[1;32m     18\u001b[0m     rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m,\n\u001b[1;32m     19\u001b[0m     rotation_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     horizontal_flip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     25\u001b[0m     fill_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m train_generator \u001b[38;5;241m=\u001b[39m datagen\u001b[38;5;241m.\u001b[39mflow_from_directory(\n\u001b[1;32m     28\u001b[0m     train_dir,\n\u001b[1;32m     29\u001b[0m     target_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m),\n\u001b[1;32m     30\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m     31\u001b[0m     class_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     33\u001b[0m val_generator \u001b[38;5;241m=\u001b[39m ImageDataGenerator(rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mflow_from_directory(\n\u001b[1;32m     34\u001b[0m     val_dir,\n\u001b[1;32m     35\u001b[0m     target_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m),\n\u001b[1;32m     36\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m     37\u001b[0m     class_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Load ResNet50 Base Model\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/legacy/preprocessing/image.py:1138\u001b[0m, in \u001b[0;36mImageDataGenerator.flow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflow_from_directory\u001b[39m(\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1122\u001b[0m     directory,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1136\u001b[0m     keep_aspect_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1137\u001b[0m ):\n\u001b[0;32m-> 1138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DirectoryIterator(\n\u001b[1;32m   1139\u001b[0m         directory,\n\u001b[1;32m   1140\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1141\u001b[0m         target_size\u001b[38;5;241m=\u001b[39mtarget_size,\n\u001b[1;32m   1142\u001b[0m         color_mode\u001b[38;5;241m=\u001b[39mcolor_mode,\n\u001b[1;32m   1143\u001b[0m         keep_aspect_ratio\u001b[38;5;241m=\u001b[39mkeep_aspect_ratio,\n\u001b[1;32m   1144\u001b[0m         classes\u001b[38;5;241m=\u001b[39mclasses,\n\u001b[1;32m   1145\u001b[0m         class_mode\u001b[38;5;241m=\u001b[39mclass_mode,\n\u001b[1;32m   1146\u001b[0m         data_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_format,\n\u001b[1;32m   1147\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1148\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39mshuffle,\n\u001b[1;32m   1149\u001b[0m         seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[1;32m   1150\u001b[0m         save_to_dir\u001b[38;5;241m=\u001b[39msave_to_dir,\n\u001b[1;32m   1151\u001b[0m         save_prefix\u001b[38;5;241m=\u001b[39msave_prefix,\n\u001b[1;32m   1152\u001b[0m         save_format\u001b[38;5;241m=\u001b[39msave_format,\n\u001b[1;32m   1153\u001b[0m         follow_links\u001b[38;5;241m=\u001b[39mfollow_links,\n\u001b[1;32m   1154\u001b[0m         subset\u001b[38;5;241m=\u001b[39msubset,\n\u001b[1;32m   1155\u001b[0m         interpolation\u001b[38;5;241m=\u001b[39minterpolation,\n\u001b[1;32m   1156\u001b[0m         dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m   1157\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/legacy/preprocessing/image.py:453\u001b[0m, in \u001b[0;36mDirectoryIterator.__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[1;32m    452\u001b[0m     classes \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 453\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subdir \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(directory)):\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, subdir)):\n\u001b[1;32m    455\u001b[0m             classes\u001b[38;5;241m.\u001b[39mappend(subdir)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/arnavkarnik/Documents/Recognition-of-Medicinal-Plant-Species-Deep-Learning-Project--Sem6/Output/train'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Define paths\n",
    "data_dir = \"/Users/arnavkarnik/Documents/Recognition-of-Medicinal-Plant-Species-Deep-Learning-Project--Sem6/Output/\"\n",
    "train_dir = data_dir + \"train\"\n",
    "val_dir = data_dir + \"val\"\n",
    "\n",
    "# Image Data Augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical')\n",
    "\n",
    "val_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical')\n",
    "\n",
    "# Load ResNet50 Base Model\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "base_model.trainable = False  # Freeze base model initially\n",
    "\n",
    "# Add Custom Layers\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(512, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(256, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output_layer = Dense(train_generator.num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=output_layer)\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks for Improvement\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=20,\n",
    "    callbacks=[reduce_lr, early_stopping],\n",
    "    verbose=1)\n",
    "\n",
    "# Evaluate Model\n",
    "y_true = np.concatenate([val_generator.classes])\n",
    "y_pred = np.argmax(model.predict(val_generator), axis=1)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=val_generator.class_indices.keys()))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
