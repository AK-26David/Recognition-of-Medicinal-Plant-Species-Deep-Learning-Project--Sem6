{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computer Vision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-image version: 0.24.0\n",
      "Frangi function parameters:\n",
      "(image, sigmas=range(1, 10, 2), scale_range=None, scale_step=None, alpha=0.5, beta=0.5, gamma=None, black_ridges=True, mode='reflect', cval=0)\n",
      "For more details, refer to the documentation at:\n",
      "https://scikit-image.org/docs/stable/api/skimage.filters.html#skimage.filters.frangi\n",
      "Found 1835 images to process\n",
      "Processed 0/1835 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0b/d5vghlcn39jb_kt0c4whkzcr0000gn/T/ipykernel_7610/2342296929.py:39: UserWarning: Use keyword parameter `sigmas` instead of `scale_range` and `scale_range` which will be removed in version 0.17.\n",
      "  venation_map = frangi(clahe_image,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100/1835 images\n",
      "Processed 200/1835 images\n",
      "Processed 300/1835 images\n",
      "Processed 400/1835 images\n",
      "Processed 500/1835 images\n",
      "Processed 600/1835 images\n",
      "Processed 700/1835 images\n",
      "Processed 800/1835 images\n",
      "Processed 900/1835 images\n",
      "Processed 1000/1835 images\n",
      "Processed 1100/1835 images\n",
      "Processed 1200/1835 images\n",
      "Processed 1300/1835 images\n",
      "Processed 1400/1835 images\n",
      "Processed 1500/1835 images\n",
      "Processed 1600/1835 images\n",
      "Processed 1700/1835 images\n",
      "Processed 1800/1835 images\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.filters import frangi\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "\n",
    "def preprocess_leaf_image(image_path, output_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Preprocess a leaf image to extract RGB, venation map, and edge map\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the leaf image\n",
    "        output_size (tuple): Size to resize images to (height, width)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (RGB image, venation map, edge map)\n",
    "    \"\"\"\n",
    "    # Read the original image\n",
    "    original_image = cv2.imread(image_path)\n",
    "    if original_image is None:\n",
    "        raise ValueError(f\"Could not read image at {image_path}\")\n",
    "    \n",
    "    # 1. RGB Image Extraction\n",
    "    rgb_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "    rgb_image = cv2.resize(rgb_image, output_size)\n",
    "    \n",
    "    # 2. Venation Map Extraction\n",
    "    # Convert to grayscale\n",
    "    gray_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2GRAY)\n",
    "    gray_image = cv2.resize(gray_image, output_size)\n",
    "    \n",
    "    # Apply CLAHE (Contrast Limited Adaptive Histogram Equalization)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    clahe_image = clahe.apply(gray_image)\n",
    "    \n",
    "    # Apply Frangi filter to enhance vein-like structures\n",
    "    # Updated parameters to be compatible with current scikit-image version\n",
    "    venation_map = frangi(clahe_image, \n",
    "                         scale_range=(1, 3), \n",
    "                         scale_step=0.5,\n",
    "                         beta=15,  # Using beta instead of beta1/beta2\n",
    "                         black_ridges=False)\n",
    "    \n",
    "    # Normalize the venation map\n",
    "    venation_map = cv2.normalize(venation_map, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "    \n",
    "    # 3. Edge Map Extraction using Canny edge detection\n",
    "    edge_map = cv2.Canny(gray_image, 50, 150)\n",
    "    \n",
    "    return rgb_image, venation_map, edge_map\n",
    "\n",
    "def process_dataset(dataset_path, output_dir):\n",
    "    \"\"\"\n",
    "    Process an entire dataset of leaf images\n",
    "    \n",
    "    Args:\n",
    "        dataset_path (str): Path to the dataset directory\n",
    "        output_dir (str): Directory to save processed images\n",
    "    \"\"\"\n",
    "    # Create output directories if they don't exist\n",
    "    os.makedirs(os.path.join(output_dir, 'rgb'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'venation'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'edge'), exist_ok=True)\n",
    "    \n",
    "    # Get all image files\n",
    "    image_extensions = ['*.jpg', '*.jpeg', '*.png']\n",
    "    image_files = []\n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(glob(os.path.join(dataset_path, '**', ext), recursive=True))\n",
    "    \n",
    "    print(f\"Found {len(image_files)} images to process\")\n",
    "    \n",
    "    for idx, image_path in enumerate(image_files):\n",
    "        try:\n",
    "            # Extract class from path (assuming dataset structure: dataset/class/image.jpg)\n",
    "            # Handle paths with spaces correctly\n",
    "            path_parts = os.path.normpath(image_path).split(os.sep)\n",
    "            class_name = path_parts[-2]  # Assuming last directory is the class name\n",
    "            file_name = os.path.basename(image_path)\n",
    "            \n",
    "            # Create class directories if they don't exist\n",
    "            os.makedirs(os.path.join(output_dir, 'rgb', class_name), exist_ok=True)\n",
    "            os.makedirs(os.path.join(output_dir, 'venation', class_name), exist_ok=True)\n",
    "            os.makedirs(os.path.join(output_dir, 'edge', class_name), exist_ok=True)\n",
    "            \n",
    "            # Process the image\n",
    "            rgb_image, venation_map, edge_map = preprocess_leaf_image(image_path)\n",
    "            \n",
    "            # Save processed images\n",
    "            cv2.imwrite(os.path.join(output_dir, 'rgb', class_name, file_name), \n",
    "                        cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR))\n",
    "            cv2.imwrite(os.path.join(output_dir, 'venation', class_name, file_name), venation_map)\n",
    "            cv2.imwrite(os.path.join(output_dir, 'edge', class_name, file_name), edge_map)\n",
    "            \n",
    "            if idx % 100 == 0:\n",
    "                print(f\"Processed {idx}/{len(image_files)} images\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_path}: {e}\")\n",
    "\n",
    "def visualize_preprocessing(image_path):\n",
    "    \"\"\"\n",
    "    Visualize the preprocessing steps for a single image\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the leaf image\n",
    "    \"\"\"\n",
    "    rgb_image, venation_map, edge_map = preprocess_leaf_image(image_path)\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(rgb_image)\n",
    "    plt.title('RGB Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(venation_map, cmap='gray')\n",
    "    plt.title('Venation Map')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(edge_map, cmap='gray')\n",
    "    plt.title('Edge Map')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Function to check scikit-image version and available parameters\n",
    "def check_frangi_parameters():\n",
    "    \"\"\"\n",
    "    Print the available parameters for the frangi function\n",
    "    \"\"\"\n",
    "    import inspect\n",
    "    from skimage import __version__ as skimage_version\n",
    "    \n",
    "    print(f\"scikit-image version: {skimage_version}\")\n",
    "    print(\"Frangi function parameters:\")\n",
    "    print(inspect.signature(frangi))\n",
    "    print(\"For more details, refer to the documentation at:\")\n",
    "    print(\"https://scikit-image.org/docs/stable/api/skimage.filters.html#skimage.filters.frangi\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Check frangi parameters to diagnose issues\n",
    "    check_frangi_parameters()\n",
    "    \n",
    "    # Path to the Mendeley leaf dataset\n",
    "    dataset_path = \"/Users/arnavkarnik/Documents/Recognition-of-Medicinal-Plant-Species-Deep-Learning-Project--Sem6/Medicinal Leaf Dataset/Segmented Medicinal Leaf Images\"\n",
    "    output_dir = \"/Users/arnavkarnik/Documents/Recognition-of-Medicinal-Plant-Species-Deep-Learning-Project--Sem6/Output\"\n",
    "    \n",
    "    # Process the entire dataset\n",
    "    process_dataset(dataset_path, output_dir)\n",
    "    \n",
    "    # Alternatively, visualize preprocessing for a single image\n",
    "    # sample_image = \"/Users/arnavkarnik/Documents/Recognition-of-Medicinal-Plant-Species-Deep-Learning-Project--Sem6/Medicinal Leaf Dataset/Segmented Medicinal Leaf Images/Punica Granatum (Pomegranate)/PG-S-022.jpg\"\n",
    "    # visualize_preprocessing(sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 25) (1539707832.py, line 25)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 25\u001b[0;36m\u001b[0m\n\u001b[0;31m    base_dir = \"/Users/arnavkarnik/Documents/Recognition-of-Medicinal-Plant-Species-Deep-Learning-Project--Sem6/Output\"\"\u001b[0m\n\u001b[0m                                                                                                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 25)\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input, Concatenate, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Enable GPU memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Paths to the preprocessed data\n",
    "base_dir = \"/Users/arnavkarnik/Documents/Recognition-of-Medicinal-Plant-Species-Deep-Learning-Project--Sem6/Output\"\n",
    "rgb_dir = os.path.join(base_dir, \"rgb\")\n",
    "venation_dir = os.path.join(base_dir, \"venation\")\n",
    "edge_dir = os.path.join(base_dir, \"edge\")\n",
    "\n",
    "# Image dimensions\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Custom data generator for multi-input model\n",
    "class MultiInputDataGenerator:\n",
    "    def __init__(self, rgb_generator, venation_generator, edge_generator):\n",
    "        self.rgb_generator = rgb_generator\n",
    "        self.venation_generator = venation_generator\n",
    "        self.edge_generator = edge_generator\n",
    "        self.n = len(self.rgb_generator)\n",
    "        self.class_indices = self.rgb_generator.class_indices\n",
    "        self.classes = self.rgb_generator.classes\n",
    "        self.num_classes = len(self.class_indices)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        rgb_batch = self.rgb_generator[idx]\n",
    "        venation_batch = self.venation_generator[idx]\n",
    "        edge_batch = self.edge_generator[idx]\n",
    "        \n",
    "        # Get the images and labels\n",
    "        x_rgb = rgb_batch[0]\n",
    "        x_venation = venation_batch[0]\n",
    "        # Convert grayscale to 3-channel (required for ResNet input)\n",
    "        x_venation = np.stack([x_venation[:,:,:,0]]*3, axis=-1)\n",
    "        \n",
    "        x_edge = edge_batch[0]\n",
    "        # Convert grayscale to 3-channel\n",
    "        x_edge = np.stack([x_edge[:,:,:,0]]*3, axis=-1)\n",
    "        \n",
    "        y = rgb_batch[1]  # Labels are the same for all generators\n",
    "        \n",
    "        return [x_rgb, x_venation, x_edge], y\n",
    "    \n",
    "    def reset(self):\n",
    "        self.rgb_generator.reset()\n",
    "        self.venation_generator.reset()\n",
    "        self.edge_generator.reset()\n",
    "\n",
    "# Data augmentation for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Only rescaling for validation data\n",
    "val_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# RGB data generators\n",
    "train_rgb_generator = train_datagen.flow_from_directory(\n",
    "    rgb_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "val_rgb_generator = val_datagen.flow_from_directory(\n",
    "    rgb_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Venation data generators\n",
    "train_venation_generator = train_datagen.flow_from_directory(\n",
    "    venation_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    color_mode='grayscale',\n",
    "    subset='training',\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "val_venation_generator = val_datagen.flow_from_directory(\n",
    "    venation_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    color_mode='grayscale',\n",
    "    subset='validation',\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Edge data generators\n",
    "train_edge_generator = train_datagen.flow_from_directory(\n",
    "    edge_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    color_mode='grayscale',\n",
    "    subset='training',\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "val_edge_generator = val_datagen.flow_from_directory(\n",
    "    edge_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    color_mode='grayscale',\n",
    "    subset='validation',\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Create combined generators\n",
    "train_generator = MultiInputDataGenerator(train_rgb_generator, train_venation_generator, train_edge_generator)\n",
    "val_generator = MultiInputDataGenerator(val_rgb_generator, val_venation_generator, val_edge_generator)\n",
    "\n",
    "# Build the three-stream model\n",
    "def build_multi_stream_resnet50_model(num_classes):\n",
    "    # RGB stream\n",
    "    rgb_input = Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3), name=\"rgb_input\")\n",
    "    rgb_base = ResNet50(weights='imagenet', include_top=False, input_tensor=rgb_input)\n",
    "    \n",
    "    # Freeze early layers of ResNet\n",
    "    for layer in rgb_base.layers[:-30]:  # Freeze all but the last 30 layers\n",
    "        layer.trainable = False\n",
    "    \n",
    "    rgb_features = GlobalAveragePooling2D()(rgb_base.output)\n",
    "    \n",
    "    # Venation stream\n",
    "    venation_input = Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3), name=\"venation_input\")\n",
    "    venation_base = ResNet50(weights='imagenet', include_top=False, input_tensor=venation_input)\n",
    "    \n",
    "    # Freeze early layers of ResNet\n",
    "    for layer in venation_base.layers[:-30]:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    venation_features = GlobalAveragePooling2D()(venation_base.output)\n",
    "    \n",
    "    # Edge stream\n",
    "    edge_input = Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3), name=\"edge_input\")\n",
    "    edge_base = ResNet50(weights='imagenet', include_top=False, input_tensor=edge_input)\n",
    "    \n",
    "    # Freeze early layers of ResNet\n",
    "    for layer in edge_base.layers[:-30]:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    edge_features = GlobalAveragePooling2D()(edge_base.output)\n",
    "    \n",
    "    # Combine features from all streams\n",
    "    combined_features = Concatenate()([rgb_features, venation_features, edge_features])\n",
    "    \n",
    "    # Classification head\n",
    "    x = Dense(512, activation='relu')(combined_features)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    # Create the model\n",
    "    model = Model(inputs=[rgb_input, venation_input, edge_input], outputs=outputs)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build model\n",
    "num_classes = len(train_rgb_generator.class_indices)\n",
    "model = build_multi_stream_resnet50_model(num_classes)\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Callbacks\n",
    "checkpoint_path = \"best_model.h5\"\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    checkpoint_path,\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=5,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "EPOCHS = 50\n",
    "steps_per_epoch = len(train_generator)\n",
    "validation_steps = len(val_generator)\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=[model_checkpoint, early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save the final model\n",
    "model.save(\"final_venation_model.h5\")\n",
    "\n",
    "# Plot training results\n",
    "def plot_training_results(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Accuracy plot\n",
    "    ax1.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    ax1.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    ax1.set_title('Model Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Loss plot\n",
    "    ax2.plot(history.history['loss'], label='Training Loss')\n",
    "    ax2.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    ax2.set_title('Model Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_results.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "plot_training_results(history)\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(model, generator, class_names):\n",
    "    # Get predictions\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    generator.reset()\n",
    "    for i in range(len(generator)):\n",
    "        x, y = generator[i]\n",
    "        batch_pred = model.predict(x)\n",
    "        batch_pred_classes = np.argmax(batch_pred, axis=1)\n",
    "        batch_true_classes = np.argmax(y, axis=1)\n",
    "        \n",
    "        y_true.extend(batch_true_classes)\n",
    "        y_pred.extend(batch_pred_classes)\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Get class names\n",
    "class_names = list(train_rgb_generator.class_indices.keys())\n",
    "\n",
    "# Evaluate the model\n",
    "report = evaluate_model(model, val_generator, class_names)\n",
    "\n",
    "# Save classification report\n",
    "import json\n",
    "with open('classification_report.json', 'w') as f:\n",
    "    json.dump(report, f)\n",
    "\n",
    "print(f\"Training complete. Model and results saved.\")\n",
    "\n",
    "# Function to visualize the attention to venation patterns\n",
    "def visualize_feature_importance(model, sample_image_path, output_dir=\"feature_importance\"):\n",
    "    \"\"\"\n",
    "    Visualize which parts of the leaf the model pays attention to, especially venation patterns\n",
    "    \"\"\"\n",
    "    import cv2\n",
    "    from tensorflow.keras.preprocessing import image\n",
    "    from tf_keras_vis.gradcam import Gradcam\n",
    "    from tf_keras_vis.utils import normalize\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load and preprocess a sample image\n",
    "    rgb_img = cv2.imread(os.path.join(rgb_dir, sample_image_path))\n",
    "    rgb_img = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2RGB)\n",
    "    rgb_img = cv2.resize(rgb_img, (IMG_HEIGHT, IMG_WIDTH))\n",
    "    rgb_img_array = np.expand_dims(rgb_img/255.0, axis=0)\n",
    "    \n",
    "    venation_img = cv2.imread(os.path.join(venation_dir, sample_image_path), cv2.IMREAD_GRAYSCALE)\n",
    "    venation_img = cv2.resize(venation_img, (IMG_HEIGHT, IMG_WIDTH))\n",
    "    venation_img_array = np.stack([venation_img/255.0]*3, axis=-1)\n",
    "    venation_img_array = np.expand_dims(venation_img_array, axis=0)\n",
    "    \n",
    "    edge_img = cv2.imread(os.path.join(edge_dir, sample_image_path), cv2.IMREAD_GRAYSCALE)\n",
    "    edge_img = cv2.resize(edge_img, (IMG_HEIGHT, IMG_WIDTH))\n",
    "    edge_img_array = np.stack([edge_img/255.0]*3, axis=-1)\n",
    "    edge_img_array = np.expand_dims(edge_img_array, axis=0)\n",
    "    \n",
    "    # Create a GradCAM instance\n",
    "    gradcam = Gradcam(model)\n",
    "    \n",
    "    # Generate gradcam for each input\n",
    "    # Note: This is a simplified approach - you may need to adapt this for multi-input models\n",
    "    \n",
    "    # Display the results\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    plt.subplot(3, 2, 1)\n",
    "    plt.imshow(rgb_img)\n",
    "    plt.title('Original RGB Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(3, 2, 3)\n",
    "    plt.imshow(venation_img, cmap='gray')\n",
    "    plt.title('Venation Map')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(3, 2, 5)\n",
    "    plt.imshow(edge_img, cmap='gray')\n",
    "    plt.title('Edge Map')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Save the visualization\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f\"feature_importance_{os.path.basename(sample_image_path)}\"), dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Uncomment to visualize feature importance for a sample image\n",
    "# sample_image_path = \"class_name/image_filename.jpg\"  # Replace with an actual image path\n",
    "# visualize_feature_importance(model, sample_image_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
